{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hvLjZu9gt7i",
        "outputId": "5dcdb599-ac4c-474d-e45d-70b37528a987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m122.9/130.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-community\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.68)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.4)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.1)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.27 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "#Instalando bibliotecas necessárioas\n",
        "\n",
        "!pip install -qU langchain-groq\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fazendo conexão com a API e pegando o modelo que será usado\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = \"Token\"\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
      ],
      "metadata": {
        "id": "_dkWjU5Ig5L_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "-xK0SiYQhLZf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta é a função que vai guardar o chat na memória para que a IA lembre das mensagens\n",
        "\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain.memory.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "memory_store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in memory_store:\n",
        "        memory_store[session_id] = ChatMessageHistory()\n",
        "    return memory_store[session_id]"
      ],
      "metadata": {
        "id": "fR0fSZqRiffC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fazendo um teste, dizendo para a IA quem sou eu e minha nacionalidade\n",
        "#É possível ver que ele me respondeu com base no que eu disse\n",
        "\n",
        "runnable_with_history = RunnableWithMessageHistory(\n",
        "    runnable= llm,\n",
        "    get_session_history=get_session_history,\n",
        ")\n",
        "\n",
        "runnable_with_history.invoke(\n",
        "    [HumanMessage(content=\"hi - im Marcus Nunes. I'm Brazilian!\")],\n",
        "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "id": "MkEwxVshhPnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#É possível ver aqui também que ele lembra quem eu sou\n",
        "\n",
        "runnable_with_history.invoke(\n",
        "    [HumanMessage(content=\"hi - do you remember my name and where I come from?\")],\n",
        "    config={\"configurable\": {\"session_id\": \"1\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIhkYw7Gr6Yi",
        "outputId": "89dfc765-b9aa-4004-b61a-fd947c9d8339"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Olá Marcus! Eu lembro que você é brasileiro. Você veio falar comigo apenas faz um momento atrás. Eu não tenho uma memória de longo prazo, então eu não posso lembrar de tudo que aconteceu anteriormente, mas eu lembro que você disse que é brasileiro e que se chama Marcus Nunes.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 115, 'total_tokens': 193, 'completion_time': 0.158356977, 'prompt_time': 0.006739061, 'queue_time': 0.213328931, 'total_time': 0.165096038}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8ab2e50475', 'finish_reason': 'stop', 'logprobs': None}, id='run--d49bbe4a-1c7d-40a7-8527-5b3584bf5779-0', usage_metadata={'input_tokens': 115, 'output_tokens': 78, 'total_tokens': 193})"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aqui eu passo o promp template, onde eu posso configurar as respostas da minha IA, decidir o que ela vai falar e como vai falar\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\",\n",
        "         \"You are an assistat who speaks in {language}. Respond in 20 words or fewer\", #Falei pra ela que ela é um assistente que fala em uma lingua que eu passar, e pra responder em 20 palavras ou menos\n",
        "         ),\n",
        "        MessagesPlaceholder(variable_name=\"history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "runnable = prompt | llm\n",
        "\n",
        "runnable_with_history = RunnableWithMessageHistory(\n",
        "    runnable= runnable,\n",
        "    get_session_history=get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"history\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "06fvddx7hyJE"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runnable_with_history.invoke(\n",
        "    {\"language\": \"Portuguese\", \"input\": \"Hi im Marcus, I'm 21 years old!\"}, # Aqui testo de novo a IA com o parâmetro de língua. Escolhi a lingua portuguesa\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}}, # Definindo uma nova sessão, a memória da IA reinicia e volta do zero\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y-atf11hyL5",
        "outputId": "16ce68bb-e8ad-4e69-804e-a70d9001b17a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Olá Marcus! 21 anos, é uma idade incrível! (Hello Marcus! 21 years old, what an incredible age!)', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 63, 'total_tokens': 92, 'completion_time': 0.051840697, 'prompt_time': 0.003706192, 'queue_time': 0.213876811, 'total_time': 0.055546889}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8ab2e50475', 'finish_reason': 'stop', 'logprobs': None}, id='run--e0a753eb-e3a1-4370-87ae-0b25db63dd23-0', usage_metadata={'input_tokens': 63, 'output_tokens': 29, 'total_tokens': 92})"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable_with_history.invoke(\n",
        "    {\"language\": \"Portuguese\", \"input\": \"What was my name and my age?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}},\n",
        ")\n",
        "\n",
        "#Aqui é possível ver que a IA lembrou de novo quem eu era graças a função de armazenamento na memória"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMS-Z0tIhsG-",
        "outputId": "66062c4a-7926-45e1-a45e-59ad7a67e1b8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Seu nome é Marcus e você tem 21 anos.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 109, 'total_tokens': 122, 'completion_time': 0.017333333, 'prompt_time': 0.006408381, 'queue_time': 0.214515321, 'total_time': 0.023741714}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'finish_reason': 'stop', 'logprobs': None}, id='run--47252af3-0e91-4bc7-afdc-7cb04d24dff5-0', usage_metadata={'input_tokens': 109, 'output_tokens': 13, 'total_tokens': 122})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Aqui testo o capacidade de resumo da IA passadno a história dos três porquinhos. Ao mesmo tempo que testo essa habilidade, eu testo a memória também na célula abaixo dessa\n",
        "\n",
        "runnable_with_history.invoke(\n",
        "    {\"language\": \"Portuguese\", \"input\": \"\"\"Era uma vez três porquinhos que decidiram construir suas casas para se proteger do lobo mau. O primeiro porquinho fez sua casa de palha, porque era rápido e fácil. O segundo construiu a casa de madeira, um pouco mais forte. O terceiro, mais cuidadoso, fez sua casa de tijolos, bem resistente.\n",
        "\n",
        "Um dia, o lobo chegou e soprou a casa de palha, que caiu facilmente. O porquinho correu para a casa de madeira. O lobo soprou com força, e a casa de madeira também desabou. Os dois porquinhos correram para a casa de tijolos.\n",
        "\n",
        "O lobo tentou soprar, soprar, mas a casa de tijolos não caiu. Cansado, ele desistiu. Os três porquinhos aprenderam que, com esforço e cuidado, a proteção é mais forte.\n",
        "\n",
        "Fim.\"\"\"},\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVJUTColtMfw",
        "outputId": "c83d8dc4-c7fe-4e41-dfd0-cc6ec09d6e33"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Que história agradável! É um ensinamento valioso para todos. Proteção é mais forte com esforço e cuidado.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 352, 'total_tokens': 382, 'completion_time': 0.070467278, 'prompt_time': 0.020841706, 'queue_time': 0.217917103, 'total_time': 0.091308984}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8ab2e50475', 'finish_reason': 'stop', 'logprobs': None}, id='run--91a3b7fd-41e3-4972-a485-32e3affff1df-0', usage_metadata={'input_tokens': 352, 'output_tokens': 30, 'total_tokens': 382})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runnable_with_history.invoke(\n",
        "    {\"language\": \"Portuguese\", \"input\": \"faça um resumo de 1 linha da história que te contei\"},\n",
        "    config={\"configurable\": {\"session_id\": \"2\"}},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXG7JVHHtqs7",
        "outputId": "17fe9f2d-adf8-4eaf-c0a9-54dfdabc27ea"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Três porquinhos construiram casas com diferentes materiais, e a casa de tijolos foi a única a resistir ao ataque do lobo mau.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 406, 'total_tokens': 445, 'completion_time': 0.061671784, 'prompt_time': 0.024149708, 'queue_time': 0.21627752, 'total_time': 0.085821492}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_510c177af0', 'finish_reason': 'stop', 'logprobs': None}, id='run--57ea9481-a52d-4cfb-a8cc-1e029601d168-0', usage_metadata={'input_tokens': 406, 'output_tokens': 39, 'total_tokens': 445})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hp11sCEotzWU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}